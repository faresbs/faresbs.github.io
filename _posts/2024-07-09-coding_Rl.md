---
layout: post
title: Hand on RL algorithms
subtitle: This notebook is a tutorial to explain and showcase how to use RL algorithms like Q learning (model-free version and the DQN version), Sarsa, MC and how they differ, using Pytorch and OpenAI Gymnasium library. This notebook will give you a straightford overview of how RL algorithms work with real examples. I deliberately made the code redundant to showcase the differences and similarities of the different algorithms.
cover-img: /assets/img/posts/images_2017-01-02/cover.jpg
thumbnail-img: /assets/img/posts/images_2017-01-02/logistic_regression_model.png
share-img: /assets/img/posts/images_2017-01-02/cover.jpg
tags: [AI, ML, Reinforcement Learning]
author: Fares Ben Slimane
date:   2024-07-09
---


##### Requirements :

*   Pytorch: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
*   Gymnasium: [https://gymnasium.farama.org/](https://gymnasium.farama.org/)

## Sections

1. **Algorithm overview**

2. **Q learning**

3. **Double Q Learning**

4. **Monte Carlo**

5. **Sarsa**

6. **Deep Q Network**

7. **Experimenting with other Games from the library**


## Cliff walking:  Task Definition 

The agent has to decide between 4 actions - Right, Left, Bottom or Up.

As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action.

In this task, Each time step incurs -1 reward, unless the player stepped into the cliff, which incurs -100 reward. Getting to the target gives you a +100 reward. Obviously, the goal is to get to the target point with more rewards. 
The space is represented by a 12 x 4 (48 observations) with 3 x 12 + 1 possible states. The agent observes the position from the environment and choose an action from 4 possible actions. 

You can learn more about it here:
https://gymnasium.farama.org/environments/toy_text/cliff_walking/

In this Notebook, we are going to experiment with multiple RL algorithms: 

1) Q learning: (A) model-free Q learning (classical one) which uses a Q table to predict the expected value, (B) Double Q learning to fix the problem of maximization bias, and (C) Deep Q Network, which uses a network to predict the expected value for each action, given the input state. The action with the highest expected value is then chosen. 

2) Monte Carlo

3) Sarsa


## TD (Temporal difference learning)

To Explain Q-learning, we need first to clarify what is Temporal Difference (TD), undoubtedly the most central idea in Reinforcement learning. TD is a combination of both monte carlo and dynamic programming (You can find more about them here). Like Monte Carlo, TD learns directly from <i> experience </i> without needing to model the environment's dynamics. Like DP, TD use <i> bootstrapping </i> to estimate the values of subsequent states to determine the value of the current state. Essentially, it involves leveraging "estimated" information from future states to improve our understanding of the current state without waiting for the "actual" outcome. 

Both TD and Monte Carlo use experience following some policy $\pi$ to update their estimate for value $V(S_t)$. Whereas, MC methods need to wait until the end of the episode to determine the total return (or Reward $R$) to predict $V(S_t)$, TD on the other hand just wait until the next time step $t+1$ to form a <i>target</i> by updating $R_{t+1}$ and estimating $V(S_{t+1})$. We call this TD(0) or one step TD where the target is based solely on the next step $t+1$ outcome. This is a special case of TD($\gamma$) with $\gamma$ time steps. In a nutshell, Monte Carlo approach require us to wait for the final outcome to estimate the current state because we don't know yet the "true return" of that episode. On the other hand, TD waits for the $\gamma$ next steps or simply the next step $t_1$.

$$ V(S_t) \leftarrow V(S_t) + \alpha \left[ (R_{t+1} + \gamma V(S_{t+1}) ) - V(S_t) \right] $$


$V(S_t)$ represents the value function for current state $S_t$ that estimate . 

$\alpha$ is the learning rate (a positive constant).

$R_{t+1}$ is the immediate reward received after transitioning from state $S_t$ to state $S_{t+1}$

$\gamma$ is the discount factor (a value between 0 and 1) that accounts for future rewards.

DP methods try to solve problems by breaking them down into smaller subproblems and solving those subproblems first.
Like DP, TD use bootstrapping to update estimates based on other estimates—to iteratively improve our understanding of the overall problem. The future reward $v_\pi(S_t)$ from the current state $S_t$ and following the policy $\pi$ is the current reward plus the next step future reward $v_\pi(S_{t+1})$. Note that here the current reward is actually $R_{t+1}$ and not $R_t$, because it is the observed reward from transitioning from $S_t$ to $S_{t+1}$ using a policy. 

$$ v_\pi(S_t) = \mathop{\mathbb{E}} [G_t | S_t] = \mathop{\mathbb{E}} [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t] $$

The DP target is an estimate not because of the expected values $V(S_t)$ (provided by a model of the environment), but because the "actual or optimal" $v_\pi(S_{t+1})$ value is not known yet and therefore we rely on the estimate $V(S_{t+1})$.

$$ (R_{t+1} + \gamma V(S_{t+1}) ) - V(S_t) $$

We refer to this term as the TD error that we are going to be using to update $V(S_t)$ of the current state by multplying it by a small step $\alpha$ value. This Equation can be understood by saying that we are getting the TD error between the current prediction or estimation $V(S_t)$ and the better estimatation  $R_{t+1} + \gamma V(S_{t+1})$, which depends on the next reward and the next estimation state. We say that it is a "better estimate" because it is relying on the observed reward $R_{t+1}$ when we transition to the next time step.

### Why TD is better?

TD methods have an advantage over DP methods because they do not require a model of the environment (of all its actual rewards and probability distributions). Also, TD is better than MC, because they can be implemented in an online, incremental fasion, as it can learn one guess from the next without waiting for an actual outcome (only wait one time step). MC, on the other hand, must wait until the end of the episode, because only then that the return is known. Some application have very long episodes which can make training slower. This can be easily observed by comparing the code of TD methods like Q_learning / SARSA and that of Monte Carlo down below.

## Q-learning (Tabular method): Off-policy TD Algorithm

Q-learning is a model-free, value (Reward) based  and off-policy RL algorithm. The “Q” stands for <i>quality</i>, representing how valuable an action is in maximizing future rewards for a given state. It aims to maximize the value function Q, which estimates the expected cumulative reward. 

Iteratively, the agent adjusts its strategy over time to make optimal decisions in various situations. The algorithm uses a Q-Table to find the best action for each state, maximizing expected rewards.

Q-Learning computes the difference between the current Q-value and the maximum Q-value of the time step over all possible actions: 

$$ Q(s_t, a_t) = Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right) $$

Q-learning updates the Q-value using the maximum Q-value over all possible actions for the next state, which helps it focus on the best possible action every step. 

It explores aggressively, even if the current policy is different, aiming to learn the optimal policy <i>independently of the agent’s actions</i>. And this is why it is an <b>off-policy algorithm</b>.

### How Does it Work?

At each step:
You’re in a specific state (S) (a maze cell).
You choose an action (A) (e.g., move left, right, up, or down).
Based on that action, you receive a reward (cheese or poison).
You end up in the next state (S1) (a neighboring cell).
You then decide on another action (A1) for the next step.

### Algorithm 

Parameters: step size $\alpha = (0, 1]$, small greedy $\epsilon > 0$
Initilize Q(s, a), for s reprenting all states and a representing all possible actions, arbitrarily except that Q(terminal, ) = max reward value.

1. **Initialize** Q-values for all state-action pairs (Q-table).
2. **Loop over episodes**:
   - Initialize the current state: $S = S_0$.
   - **Loop over each step of the current episode**:
     - Choose an action $a$ from state $S$ using a policy derived from Q (e.g., $\epsilon-greedy$ strategy).
     - Take action $a$, observe the reward $R_t$ and the next state $S_{t+1}$.
     - Update the Q-table with the observed reward and next state values.
     - Compute the **TD error**:
       
       $$ error = (R_{t+1} + \gamma \max_a Q(S_{t+1}, a)) - Q(S_t, a_t) $$
       
     - Update the Q-value using the step size $\alpha$ and the calculated TD error:
       
      $$ Q(S_t, a_t) = Q(S_t, a_t) + \alpha \cdot error $$
     
     - Transition to the next step: $S_t \leftarrow S_{t+1}$.
   - **Repeat until the current state is terminal** (i.e., $S_t = S_{\text{end}}$).


### Q-learning python Code

```python
# Q-Learning algorithm

alpha = 0.1  # Learning rate (make sure to choose a high lr)
gamma = 0.98  # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_decay = 0.995  # Decay rate for exploration rate
min_epsilon = 0.01  # Minimum exploration rate
episodes = 600  # Number of episodes
#saving_episodes = 100

# Initialize Q-table
#Shape = (32, 4)
Q_table = np.zeros((n_observations, n_actions))

episode_durations = []

for episode in range(0, episodes):

    state, _ = env.reset()  # Get initial state
    total_reward = 0
    
    #for step in range(max_steps):
    for t in count():

        # Epsilon-greedy action selection for initial state
        # Note: make sure to do this exploit/explore inside the time step loop to make sure we are doing this on every step
        if np.random.uniform(0, 1) < epsilon:
            action = np.random.randint(0, n_actions)
        else:
            action = np.argmax(Q_table[state])
            # or action = best_next_action

        # Take action and observe the result
        next_state, reward, terminated, truncated, _ = env.step(action)

        total_reward += reward
        
        #Always pick the next best action using the current policy (and not the target policy)
        best_next_action = np.argmax(Q_table[next_state])

        # Update Q-value using the Bellman equation
        Q_table[state][action] += alpha * (reward + gamma * Q_table[next_state][best_next_action] - Q_table[state][action])
        
        state = next_state  # Move to the next state

        if terminated:
            #We can also use t for total duration instead of total reward
            episode_durations.append(t+1)
            #episode_durations.append(total_reward)
            if t % 10 == 0:
                plot_durations()
            break

    # Decay epsilon for exploration-exploitation balance
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

print("Training finished.\n")

env.close()
```

### Monte Carlo python Code

Unlike Q-learning, through Monte Carlo approach we have to update update the Q-table for the full episode trajectory at end of episode.

```python
    # Calculate returns and update Q-table
    G = 0
    cum_rewards = 0
    cum_count = 0

    for t in reversed(range(len(reward_trajectory))):

        reward = reward_trajectory[t]
        action = action_trajectory[t]
        state = state_trajectory[t]

        G = gamma * G + reward

        # first-visit MC method
        #You can use to average the total cum rewards for better smoother update
        Q_table[state][action] += alpha * (G - Q_table[state][action])
        
    # Decay epsilon for exploration-exploitation balance
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

```

## Sarsa: On-Policy TD Algorithm

Sarsa (stands for State-Action-Reward-State-Action) is another popular RL algorithm that is <b>on-policy</b>, which is used to learn a new policy for better decision-making. Unlike Q-learning, SARSA considers the action taken in the next state when updating Q-values. It updates its policy based on the actual actions taken by the agent, and thats why it an on-policy algorithm.

$$ Q(s_{t+1}, a_{t+1}) = Q(s_{t+1}, a) + \alpha \left( R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s, a) \right) $$

SARSA updates the Q-value for the current state-action pair based on the reward, the next state, and the action taken by the current policy.

### Q-learning vs Sarsa

The key difference between Q-learning and SARSA is that SARSA is an on-policy algorithm, meaning it updates the Q-values using the action actually taken by the policy, whereas Q-learning is an off-policy algorithm that updates the Q-values using the action that maximizes the Q-value.
In summary, SARSA is cautious, balancing exploration and exploitation, considering the current policy, while Q-learning boldly explores the best possible actions, for any given policy.

As can be seen, in the code difference, Q-learning: Uses epsilon-greedy for action selection at each step and updates Q-values based on the maximum Q-value for the next state.
SARSA: Uses epsilon-greedy for action selection at each step and updates Q-values based on the action actually taken in the next state.

### How Does it Work?

Imagine an agent navigating an environment (like a robot in a maze).
At each step:
It’s in a state (S).
It takes an action (A).
Receives a reward R.
Ends up in the next state (S1).
Chooses another action (A1) in the next state.
The tuple (S, A, R, S1, A1) represents SARSA.

### Algorithm 

Parameters: step size $\alpha = (0, 1]$, small greedy $\epsilon > 0$
Initilize Q(s, a), for s reprenting all states and a representing all possible actions, arbitrarily except that Q(terminal, ) = max reward value.

1. **Initialize** Q-values for all state-action pairs (Q-table).
2. **Loop over episodes**:
   - Initialize the current state: $S = S_0$.
   - Choose an action $a$ from state $S$ using a policy derived from Q (e.g., $\epsilon-greedy$ strategy).
   - **Loop over each step of the current episode**:
     - Take action $a$, observe the reward $R_t$ and the next state $S_{t+1}$.
     - Choose an action $a{t+1}$ from state $S_{t+1}$ using a policy derived from Q (e.g., $\epsilon-greedy$ strategy).
     - Update the Q-table with the observed reward and next state values.
     - Compute the **TD error**:
      $$  \text{error} = (R_{t+1} + \gamma \max_a Q(S_{t+1}, a)) - Q(S_t, a_t) $$
     - Update the Q-value using the step size $\alpha$ and the calculated TD error:
      $$  Q(S_t, a_t) = Q(S_t, a_t) + \alpha \cdot \text{error} $$
     - Transition to the next step: $S_t \leftarrow S_{t+1}$.
   - **Repeat until the current state is terminal** (i.e., $S_t = S_{\text{end}}$).

### Sarsa python Code

Instead of always picking the best next action with the max operation as we did in Q-learning, we apply the epsilon greedy approach to select the action for the next time step.

```python

# Update Q-value using the Bellman equation
        #Instead of picking the best action for the next time step, we pick the next action using our current policy function (Q-table)

        # Epsilon-greedy action selection for the next action
        if np.random.uniform(0, 1) < epsilon:
            #Explore
            next_action = np.random.randint(0, n_actions)
        else:
            #Exploit
            next_action = np.argmax(Q_table[next_state])
```

You can find the entire Notebook here: 
